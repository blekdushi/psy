---
layout: page
title: Evaluationsforschung
vo: Einführung in die Anwendungsfelder der Psychologie
date: 22.10.2020
full_title: Einführung in die Evaluationsforschung
vortragender: Schober
---

* VO: {{ page.vo }}
* Datum: {{ page.date }}
* {{ page.full_title }} ({{ page.vortragender }})

# Definition und Abgrenzung

Alltagsdefinition:
<blockquote>
Der Begriff „Evaluation“ wird zunehmend auch in der Alltagssprache verwendet, jedoch häufig nicht in diesem professionellen Sinn, sondern es wird damit lediglich ausgedrückt, dass etwas in irgendeiner Weise geprüft, bewertet oder beurteilt wurde. (Kromrey, 2001, zitiert nach DeGEval, 2016, S.15)
</blockquote>

Wissenschaftlich-professionell:
<blockquote>
Evaluationsforschung untersucht wissenschaftsgestützt unter Berücksichtigung geltender Standards die Effektivität (Ausmaß der Zielerreichung) und Effizienz (Verhältnis von Aufwand und Nutzen) von Gegenständen. (Spiel, 2003, zitiert nach Spiel et al., 2010, S.223)
</blockquote>

Beachte die Unterscheidung zwischen **Effektivität** und **Effizienz**.

Professionelle Evaluation bedeutet, dass sie systematisch, nachvollziehbar und wissenschaftsgestützt erfolgt und dokumentiert wird. Das Bewusstsein für **Qualitätskontrolle** bei Evaluationen ist in den letzten Jahren gestiegen.

**Evaluationsforschung** hat zum Ziel die Aussagefähigkeit von Evaluationen zu erhöhen.

**Abgrenzung** der Evaluationsforschung zu den folgenden Begriffen im wissenschaftlichen Anspruch und bzgl. kontinuierlich ↔ punktuell:
* _Qualitätssicherung_ stammt aus der Wirtschaft mit dem Fokus Produkte, Dienstleistungen und dahinter stehende Prozesse zu verbessern.
* _Monitoring_ ist die regelmäßige Überwachung und Begleitungen von Prozessen.
* _Bildungsmonitoring_ hat fließenden Übergang zur Evaluationsforschung:
  * Trotz des _deskriptiven_ Charakters (es liefert Rückmeldung über IST-Zustand des Bildungssystems) dient es als Grundlage für politische Entscheidungen
  * Viele Stakeholder, deshalb funktioniert die der Prozess "Evaluationsdesign → Ergebnisse → Verbesserung" nicht so gut
  * Es zieht aussagekräftige Stichproben und _testet_ ein breites Kompetenzspektrum
  * Es ist Basis für Bildungsberichte (PISA, TIMSS, PIRLS, TALIS, PIAAC), wovon PISA und TIMSS als “Evaluationen” des Bildungssystem gesehen wreden
  * Beispiel zur Kompetenzarmut bei österreichischen Schüler\*innen: _Individualeffekte_ bei Bildungsstand und Herkunft der Eltern (höhere Kompetenzarmut bei schlechter Bildung der Eltern mit Migrationshintergrund) und _Kompositionseffekte_ bei Unterrichtsbedingungen der Schule (Höhere Kompetenzarmut bei großen Klassen mit wenigen Lehrer\*innen).

**Evaluationsgegenstände** können sein (Spiel et al., 2010, S.224):
* Programme, Maßnahmen, Interventionen
* Einrichtungen und Organisationsteile
* Techniken und Methoden
* Produkte und Personen
* Prozesse und Netzwerke

# Ziele, Funktionen und Nutzen

Je nach Art der Evaluationen möchte man folgende **Ziele** erreichen. Evaluationen können mehrere dieser Ziele mit unterschiedlicher Gewichtung verfolgen (Spiel et al., 2010, S.226):
* _Baseline Evaluation_ - IST-Stand beschreiben
* _Prospektive Evaluation_ - Abschätzung von Machbarkeit und (Neben-)Effekten geplanter Maßnahmen
* _Formative Evaluation_ - Bewertet nicht die Maßnahme selbst, sondern die Umsetzung einer Maßnahme
* _Summitve Evaluation_ - Bewertung der Wirksamkeit einer Maßnahme nach ihrer Umsetzung
* _Programmeffizienz_ - Bewertet die Kosten-Nutzen-Relation einer Maßnahme
* _Impact Evaluation_ - Bewertet die nachhaltigen Effekte einer Maßnahme

In Wechselwirkung mit den Zielen stehen auch die **Funktionen** von Evaluationen. Diese lassen sich nicht klar von einander abgrenzen und haben fließende Übergänge. Sie werden in der Literatur nicht einheitlich definiert. Die folgende Liste ist konsoldiert aus Spiel et al.(2010, S.228-229) und Stockmann (2006, S.20):
* _Bewertung ohne klare Zielsetzung_ - Umfassende Informationssammlung, oft Widerstand bei Stakeholdern
* _Erkenntnisfunktion_ - Eigenschaften und Wirkung von Maßnahmen/Interventionen
* _Kontrollfunktion_ - Ähnlich wie Erkenntnis- und Aufklärung, mit Schwerpunkt auf Korrektheit der Umsetzung, Effizienz, Effektivität, Nebenwirkungen...
* _Dialog-/Lernfunktion_ - Ergebnisse dienen den Stakeholdern als Diskussionsgrundlage für Retrospektive und Verbesserungen bei zukünftiger Zusammenarbeit
* _Entscheidungsunterstützung_ - Vergleichen von Vor- und Nachteilen von Entscheidungsalternativen
* _Optimierungsfunktion_ - z.B. Verbessern von Prozessen, eher qualitative Daten und formative Evaluationen um Problembereiche zu entdecken
* _Strategisches Instrument_ - Der Nutzen ist weniger das Ergebnis der Evaluation, sondern bereits der, _dass_ überhaupt evaluiert wird:
  * Erhöhung von Motivation und Leistung durch das Bewusstsein, wahrgenommen zu werden
  * als (politische) Durchsetzungshilfe für Entscheidung bzw. um die Verantwortung an andere delegieren zu können
  * symbolisch/ritualisiert wegen gesetzlicher/vertraglicher Notwendigkeit

Das Wort "Ziel" kann im Kontext von Evaluationen zwei Bedeutungen haben. Eine ist der _Evaluationszweck_, also warum man überhaupt eine Evaluierung durchführt. Die andere ist das Ziel des zu evaluierenden Gegenstandes. Wenn ich z.B. ein Programm zur Förderung der Mitarbeitergesundheit in einem Unternehmen evaluieren, dass ist verbesserte Mitarbeitergesundheit das Ziel des Gegenstandes und das Verbessern der Förderungsmaßnahme das Ziel der Evaluation (Evaluationszweck).

# Ebenen

Bei der Evaluation von Maßnahmen können folgende Ebenen unterschieden werden. Diese Ebenen bauen hierarchisch aufeinander auf, sodass eine Ebene erst dann evaluiert werden kann, wenn auch die vorhergehenden evaluiert wurden. Dabei wird das Erheben von zuverlässigen Daten mit jeder Ebene schwieriger und kostspieliger (Kirkpatrick & Kirkpatrick, 2006):
* _Reaktion_ - Zufriedenheit und Akzeptanz der Teilnehmenden
* _Lernen_ - was haben die Teilnehmenden gelernt? Welche Fertigkeiten konnten sie verbessern?
* _Verhalten_ - Konnte das Erlernte auch umgesetzt werden und hatte es Einfluss auf das Verhalten?
* _Ergebnisse_ - Was hat sich auf Organisations- und Systemebene durch die Maßnahme verändert (z.B. Umsatz)?

# Dimensionen zur Unterscheidung

Evaluationen können sich anhand folgender **Dimensionen** unterscheiden (DeGEval, 2016):
* Erkenntnistheoretisches Fundament (kritisch-rational hypothesenprüfend, pragmatisch, konstruktivistisch...)
* Partizipationsvorstellungen (konkurrierenden „Anwaltsteams“, parlamentarisch-repräsentative, stark oder gemäßigt beteiligungsorientierte, basisdemokratische, ...),
* Bezugswissenschaften (Wirtschaftswissenschaft, Soziologie, Politikwissenschaft, Erziehungswissenschaft, Ethnologie, Ingenieurwissenschaft, ...),
* Zwecksetzung (z. B. Vorbereitung von Entscheidungen, Verbesserung von Organisationen oder Programmen...)
* Leistungen der Evaluation (präformativ – entwickelnd, formativ – gestaltend, summativ – bilanzierend)
* Steuerungsfaktoren (Zwecksetzungen, Werte und Interessen der Beteiligten, Ziele des evaluierten Programms, Kosten-Nutzen-Relationen...)
* Phase der Entwicklung des Gegenstands (ex ante, on going, ex post)
* Umfang und Komplexität
* Dimension des Evaluationsgegenstandes (Kontext, Struktur, Konzept, Input, Prozess oder Wirkung)
* Selbst-/Fremd-/Interne/Externe Evaluation

# Akteure und Rollen

Evaluationen können anhand der **Rolle der Evaluatior\*innen** unterschieden werden:
* _Selbstevaluation_ ↔ _Fremdevaluation_
* _Interne Evaluation_ ↔ _Externe Evaluation_

<table class="table table-bordered">
<tr><td></td><th>Selbst</th><th>Fremd</th></tr>
<tr><th>Intern</th><td>Evaluator*in an Maßnahme beteiligt</td><td>Evaluator*in aus selber Organisation, aber nicht an Maßnahme beteiligt </td></tr>
<tr><th>Fremd</th><td style="text-align:center">✘</td><td>Unabhängige Dritte (Experten)</td></tr>
</table>

An einer Evaluation sind viele Personen in **unterschiedlichen Rollen** beteiligt:
* Evaluatoren
* Adressaten - Überbegriff für alle Personen, an die die Ergebnisse gerichtet sind (Nutzer, Finanzier, Auftraggeber...)
* Nutzer - Generieren den Nutzen aus den Ergebnissen. Setzen darauf aufbauend Maßnahmen.
* Auftraggeber - manchmal mit Nutzern identisch, muss aber nicht sein (z.B. politisches Umfeld).
* Finanzier - manchmal mit Auftraggeber identisch, muss aber nicht sein.
* Beteiligte - Personen, die in die Evaluation und Datenerhebung miteinbezogen werden
* Betroffene - Personen mit wenig Einfluss, die vom Evaluationsgegenstand aber beeinflusst/benachteiligt werden oder gar von ihm keine Kenntnis haben, obwohl er in ihrem Interesse wäre. Sie sollten identifiziert und zu Beteiligten gemacht werden.

# Standards

Das **Ziel** von Standards ist Orientierung bei der Planung und Durchführung von Evaluationen zu geben und somit die Qualität zu sichern und zu verbessern. Aufgrund der Verschiedenheit von Evaluationen und ihren Kontexten, können nicht immer alle Standards eingehalten werden. Das Nichterfüllen von Standards wertet die Evaluation nicht ab, solange die Evaluatoren es begründen können.
1981 hat das Joint Committee on Standards for Educational Evaluation (JCSEE, 2021) seinen ersten Standard für die Evaluierung von Programmen veröffentlicht. Die DeGEval – Gesellschaft für Evaluation (2016) definiert 25 Standards, die sie in vier grundlegende Eigenschaften für Evaluationen kategorisiert:

**Nützlichkeit (Utility)**
  1. Identifizierung der Beteiligten und Betroffenen
  2. Klärung des Evaluationszwecks
  3. Glaubwürdigkeit und Kompetenz der Evaluator\*innen
  4. Auswahl und Umfang der Informationen
  5. Transparenz von Werten
  6. Vollständigkeit und Klarheit der Berichterstattung
  7. Rechtzeitigkeit von Evaluationen
  8. Nutzung und Nutzen von Evaluationen

**Durchführbarkeit (Feasibility)**
  1. Angemessene Verfahren
  2. Diplomatisches Vorgehen
  3. Effizienz

**Fairness (Propriety)**
  1. Formale Vereinbarungen
  2. Schutz individueller Rechte
  3. Vollständige und faire Überprüfung
  4. Unparteiische Durchführung und Berichterstattung
  5. Offenlegung der Ergebnisse

**Genauigkeit (Accuracy)**
  1. Beschreibung des Evaluationsgegenstandes
  2. Kontextanalyse
  3. Beschreibung von Zwecken und Vorgehen
  4. Angabe von Informationsquellen
  5. Valide und reliable Informationen
  6. Systematische Fehlerprüfung
  7. Analyse qualitativer und quantitativer Informationen
  8. Begründete Schlussfolgerungen
  9. Meta-Evaluation

# Berufs- und Praxisfelder

* Bildungsbereich
* Sozialwesen
* Familie
* Arbeitsmarkt
* Gesundheit
* Justiz- und Strafvollzug
* Wohn- und Städtebau
* Entwicklungspolitik
* ...

# Kompetenzen von Evaluator\*innen

* Sozialwissenschaftliches Methodenwissen (Fakten- und Handlungswissen)
* Psychologisches Fachwissen
* Schlüsselqualifikationen (z.B. Projektmanagement)
* Hohes Allgemeinwissen im inhaltlichen Bereich

Besonders das Methodenwissen ist von großer Bedeutung, weshalb Psychologen gut für die Evaluationsforschung geeignet sind.

# Durchführung von Evaluationen

Dabei stehen folgende **Fragen am Beginn einer Evaluation** (von Maßnahmen):
* Welche Ziele soll die Maßnahme (nicht die Evaluation!) erreichen
* Woran erkennt man die Zielerreichung?
* Wie sollen diese Ziele erreicht werden?
* Sind ausreichende Ressourcen vorhanden?

Welches **Untersuchungsdesign** ist geeignet _und_ machbar? Erlaubt das gewählten Design überhaupt _intern und extern_ valide Aussagen, also über Wirkfaktoren und Generalisierbarkeit der Maßnahme?
* One-Shot Case Study
* Ein-Gruppen-Pretest-Posttest-Design
* Kontrollgruppenplan mit Pre- und Posttest
* Quasi-experimentell

**Beispiel Curriculumsreform** des Medizinstudiums in Österreich 2002. Das Ziel war eine Qualitätssteigerung unter anderem mit der Idee mehr "problem-based" statt "lecture-based" zu unterrichten. Es gab jedoch noch _keine_ systematischen und begleitenden Evaluationen, welche die Wirksamkeit solcher Ideen belegen konnten. Es gab zwar Evaluationen der Praxisrelevanz und der Einstellungen zu einer Reform, die Befundlage blieb jedoch unklar, weil _nur Teilgruppen befragt_ wurden und die _Rücklaufquoten gering_ waren. Deshalb Auftrag zu einer _reformbegleitenden Evaluation_ mit folgenden zentralen Fragestellungen:
* IST-Situation - Ausmaß, in dem das (alte) Curriculum Fähigkeiten und Fertigkeiten von Ärzten vermittelt.
* Wie sind die Einstellungen zur Curriculumreform?
* Soll-Situation - welche Vorstellungen existieren zu den Inhalten einer Reform?

Eingebettet war die Reform in gesetzliche Rahmenbedingungen (Universitätsstudiengesetz, Evaluierungsverordnung...).

Der **Idealverlauf** so einer Evaluierung kann wie folgt skizziert werden:
* _Baseline_ - IST-Stand des alten Studienplans
* _Prospektiv_ - Evaluation des Konzepts des neuen Studienplans
* _Prozess_ - Evaluation der Umsetzung des neuen Studienplans
* _Outcome_ - Vergleich mit altem Studienplan
* _Impact_ - Summative Evaluation der langfristigen Auswirkungen

Es wurde eine externe, summative Selbst- (?!externe Selbstevaluation widerspricht sich doch?!) und Fremdevaluation gemacht. _Vorinterviews_ halfen die Dimensionen des Evaluationsgegenstandes zu klären. Ausgehend davon wurde eine Fragenbogenrehebung gemacht (Studierende, Turnusärzt\*innen, Lehrende und (Turnus-)Supervisor\*innen). Der Rücklauf bei Lehrenden und Supervisor\*innen lag in etwa bei einem Drittel. Erfragt wurden:

| Frage                                                   | Ergebnisse                                                                                                                                                                                                         |
| ------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Was _soll_ das Studium vermitteln und was vermittelt es | große Kluft! Bei der Frage nach dem Soll gab es Unterschiede bei Lehrenden und Studierenden.                                                                                                      |
| Was _können_ junge Mediziner\*innen?                    | Fremdeinschätzung war höher als Selbsteinschätzung. Bei der Selbsteinschätzung: umso länger das Studium her ist, umso geringer werden die vermittelnden Kompetenzen eingeschätzt (Ausnahme: naturw. Faktenwissen). |
| Welche Lernmethoden werden eingesetzt?                  | "aus Bücher lernen" dominiert. Nicht optimal, wenn Problemlösungskompetenzen nun fokussiert werden sollen                                                                                                          |
| Wie ist die Einstellung zur Reform?                     | Fast alle hielten es für wichtig. Lehrende im 1./2. Abschnitt mit der geringsten Zustimmung                                                                |

**Zentrale Ergebnisse**
* Notwendigkeit einer Reform bestätigt
* Kompetenzen der Studierenden als nicht hoch eingeschätzt
* Diskrepanz zwischen Ideal und Real - lediglich verzerrte Wahrnehmung? Wenn nicht, was sind die Ursachen
* Mit wachsender Praxiserfahrung wird die Leistung des Studiums geringer eingeschätzt
* Studierende waren bereit die Studierendenzahl zu beschränken, wenn dafür ihre Ausbildung verbessert würde
* Curriculumreform alleine zu wenig. Reformdreieck:
![Notwendigkeit von Organisations-, Personal- und Unterrichtsentwicklung]({{ "/assets/img/curriculum.svg" | relative_url }})

# Lernen aus Evaluationen

Über die Ziele der Evaluation hinaus gehend, können Stakeholder, Beteiligte und Betroffene **aus Evaluationen lernen**, vorausgesetzt die Evaluation wurde von ihnen akzeptiert und angenommen. Die Akzeptanz kann man erhöhen, indem bereits vor der Evaluation die Beweggründe und möglichen Konsequenzen kommuniziert werden. Psychologische Voraussetzungen um aus Evaluationen zu lernen:
* Eigenes Verhalten als Ursache für (Miss-)Erfolg wahrnehmen
* Verhältnis Aufwand-Ertrag berechenbar
* Positive Anreize für Zielerreichung
* Transparenz über Ziele
* Einbindung und Partizipation

**Neuere Ansätze** sehen Evaluator\*innen als "Coaches", welche die Beteiligten und Betroffenen zur Partizipation und Kooperation anregen, sodass sie sich zunehmend selbst evaluieren.

Viele Evaluationen scheitern bzw. wird kein Nutzen daraus gezogen. Um den **Nutzen von Evaluationen abrufen** zu können, braucht es:
* Willen und die Möglichkeit zur Umsetzung
* Ableitung von Konsequenzen aus den Evaluationsergebnissen
* Einbeziehung von Stakeholdern, Beteiligten und Betroffenen
* Bereitschaft aus der Evaluation zu lernen
* Bereitschaft Fehler zuzugeben - _Änderungsresistenz als Selbstwertschutz_: besonders ausgeprägt bei Personen mit:
  * hoher Verantwortung
  * hoher Entscheidungsfreiraum
  * langer Verweildauer in Position
  * Schlechter Unternehmenssiuation (starker Druck)
  * Vorwürfen durch Vorgesetzte

Nur Evaluationen mit **ausreichender Qualitätssicherung** können sinnvolle und ernstzunehmende Ergebnisse liefern:
* Standards einhalten
* qualifizierte Evaluator\*innen beauftragen!
