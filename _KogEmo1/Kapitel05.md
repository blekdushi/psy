---
layout: page
title: Kapitel 5 (inkl. Zusatztext 1)
full_title: "Gesichtsverarbeitung"
published: true
vo: Kognitions- und Emotionspsychologie 1
vortragender: Leder
semester: 2020W
---

{% include vo_header.md %}

Die Gesichtsverarbeitung ist ein Forschungsfeld der Kognitionspsychologie und seit den 1980er etabliert. Die offenen Fragen übersteigen bei weitem die seither gefundenen Ergebnisse dieser Grundlagenforschung.\
Besonders erforscht werden die komplexen Prozesse, mit denen Gesichter verarbeitet und erkannt werden. Gesichter dienen im Alltag dazu:
* Personen wiederzuerkennen
* Gemüts- und Gefühlsausdrücke zu lesen
* non-verbal zu kommunizieren

<a id="BruceYoung">
# Modell nach Bruce und Young

Dieses Subkapitel ist auch eine Zusammenfassung des **Zusatztext 1**:

<div style="background-color:lavender;margin-bottom:1em">Bruce, V., &amp; Young, A. (1986). Understanding face recognition. <i>British Journal of Psychology</i>, <i>77</i>(3), 305–327. <a href="https://doi.org/10.1111/j.2044-8295.1986.tb02199.x">https://doi.org/10.1111/j.2044-8295.1986.tb02199.x</a></div>

## Codes
In den 80ern präsentierten Bruce und Young ein konsolidiertes Modell der Gesichtsverarbeitung, das die komplexen Erkennungsprozesse in funktionale Komponenten aufteilt. Sie unterscheiden verschiedene Arten von Informationen, die wir aus Gesichtern ableiten, und nennen sie **Codes**:

Gesichter werden in Studien oft in Form von Fotos präsentiert. Der **pictorial code** ist jene Art von Information, die wir bezogen auf das Foto, auf dem wir das Gesicht sehen, ableiten. Dazu gehören Belichtung, Farbgebung, Rauschen, besondere Pose oder Ausdruck des Gesichts und ähnliches mehr. Also jene Information anhand derer wir eher die identische fotografische Abbildung des Gesichts wiedererkennen würden als das Gesicht selbst.

Der **structural code** ist jene Information über das Gesicht, die vom Medium (z.B. Foto) abstrahiert ist und die visuellen Eigenschaften des Gesichts codiert. Man unterscheidet den (1) **primal sketch**, also die grobe Einteilung des Gesehenen in Schattierungen und Formen. Die (2) **view-centred description**, jene Information über das Gesicht abhängig vom Blickwinkel. Und (3) die **expression-independent description**, jene Informationen, die vom Blickwinkel und vom Gesichtsausdruck abstrahiert werden und zu einem dreidimensionalen Modell des Gesichts beitragen.<br>
Dieser structural code ist (besonders für bekannte Gesichter) eigentlich eine Menge von Codes mit unterschiedlicher Detailtiefe. Einer beschreibt z.B. eher die Konfiguration des Gesichts, ein anderer dafür im Detail den Mund oder die Nase. Diese Codes sind miteinander verbunden. So lässt sich erklären, wieso wir manche Menschen einerseits nur anhand ihrer Augenpartie erkennen können, aber auch dann, wenn sie Sonnebrillen tragen.

**Visually derived semantic code** sind Informationen, die wir unbekannten (und bekannten) Gesichtern zuschreiben: Geschlecht, Alter, Vertrauenswürdigkeit, u.a.m.

Im Kontrast dazu stehen **Identity-specific semantic codes**. Informationen, die wir mit einer uns bekannten Person verknüpfen. Diese spielen auch eine Rolle beim Wiedererkennen von Gesichtern. Das Gefühl ein Gesicht wiedererkannt zu haben ist relativ stark, wenn man identity-specific semantic codes zu diesem Gesicht abrufen kann.

Die Namen zu wiederkannten Gesichtern sind **Name codes**. Der Grund warum Namen nicht zu identity-specific semantic codes gezählt werden ist, dass Namen eben keinen semantischen Gehalt haben. Das erklärt auch, wieso sie schwieriger zu merken sind.

**Expression codes** sind Informationen über den Ausdruck, die wir sowohl von unbekannten als auch von bekannten Gesichtern ableiten.

Die Bewegungen von Mund, Lippen, Zunge, während jemand mit uns spricht, erzeugen **Facial speech codes** und können den Informationsgehalt des Gesprochenen beeinflussen.

Bruce und Young nehmen an, dass expression codes und facial speech codes keinen Einfluss auf das Wiedererkennen von Gesichtern nehmen.

## Verarbeitungseinheiten

Neben den obigen beschriebenen Arten von Information, die bei der Gesichtverarbeitung entstehen und verwendet werden, unterscheiden Bruce und Young auch _funktionale Einheiten_, welche die Verarbeitungsschritte dieser Informationen repräsentieren. Die Einheiten sind so voneinander abgegrenzt, dass sie voneinander getrennt funktionieren können (z.B. durch Versuchsanordnung oder durch Gehirnverletzungen). Im Folgenden werden diese funktionalen Einheiten zusammengefasst:

Das **Structural encoding** produziert _view-centred_ und _expression-independent_ descriptions. _View-centred_ descriptions bilden den Input für **expression analysis** und **facial speech analysis**.

 Die **Face recognition units (FRU)** speichern _strucutral codes_ über bereits bekannte Gesichter. Diese gleicht sie mit den wahrgenommenen  _expression-independent_ descriptions (=Input) ab und erkennt so Gesichter wieder. FRUs können geprimed sein, wenn wir Personen erwarten zu sehen oder vor kurzem erst gesehen haben.

Für jede bekannte Person speichern wir _identity-specific semantic codes_ in einem **person identity node (PIN)**. Das durch das FRU erkannte Gesicht dient als Schlüssel um auf die Informationen über die Person (zu der das Gesicht gehört) zugreifen zu können.

Sobald wir das Gesicht erkannt (FRU) und Zugang zu den Informationen zur Person (PIN) haben, können wir in der Regel auch die Namen einer Person über die Einheit **Name generation** abrufen.

Eine besondere Rolle hat das **Cognitive system**. Es steht für "all die anderen" kognitiven Prozesse und Informationen (Denken, Erinnerungen, Schlussfolgern, Problemlösen...), die ebenfalls bei der Personenerkennung Input liefern. Bruce und Young sehen die _person identity nodes_ streng genommen als Teil des kognitiven Systems, haben sie aber in ihrem Modell prominenter platziert, weil ihre Rolle so einen wichtigen Stellenwert hat. Das _cognitive system_ erfüllt drei Funktionen:
1. Entscheidungen treffen
2. Erinnerungen & Assoziationen bereitstellen
3. Aufmerksamkeit lenken

Eine ähnliche Stellung hat das **directed visual processing**. Das gezielte Richten von Aufmerksamkeit auf Merkmale. Wenn uns jemand bittet ein Gesicht gut einzuprägen, werden unsere kognitiven Prozesse etwas anders ablaufen, als wenn wir uns ein Gesicht unbewusst und automatisch bei einer Party merken.

Teil des kognitiven Systems sind auch _visually derived semantic codes_, obwohl die Autoren explizit offen lassen, dass diese in zukünftigen Modellen auch spezifiziert und in eigene Einheiten hervorgehoben werden könnten.

Mit dem Modell von Bruce und Young lassen sich folgende Phänomene erklären:
* Wir sehen ein Gesicht, das uns bekannt vorkommt, aber wir wissen nicht wer und woher (FRU hat funktioniert, aber kein Zugriff auf PIN)
* Wir erkennen die Person (PIN) und können ihre identitiy-specific semantic codes abrufen, aber den Namen nicht (kein Zugriff auf _name generation_)
* Wir sind schneller _identity-specific semantic codes_ abzurufen als den Namen zu nennen
* Wir sind schneller eine Person wiederzuerkennen als semantische Informationen über sie abzurufen
* Wir sind unsicher, ob es sich bei einem Gesicht um eine bekannte Person handelt oder nicht (kognitives System)
* Wir erkennen ein Gesicht als uns unbekannt, trotz verblüffender Ähnlichkeit mit einem Bekannten (kognitives System)
* Aufgrund von Gehirnverletzungen und/oder Störungen konnten Patienten...
  * ...unbekannte Gesichter matchen (_structural codes_), aber bekannte Gesichter nicht wiedererkennen und umgekehrt
  * ...das Gesicht wiedererkennen, aber _expression codes_ nicht  lesen und umgekehrt
  * ...Gesichter nicht erkennen, aber _facial speech codes_

Es erklärt aber auch, warum folgende Phänomene nicht auftreten, z.B. passiert es uns nie, dass uns der Name zu einem Gesicht einfällt, aber nicht wer diese Person eigentlich ist (deshalb ist PIN als Voraussetzung für _name generation_)

## Einschränkungen

* Die Verarbeitungseinheiten sind unterschiedlich genau spezifiziert: FRU genauer als _structural encoding_, dieses wiederum genauer als das _cognitive system_.
* Gesichter werden leicht als solches erkannt. Trotzdem enthält das Modell keine vorgeschaltete "Recognize as face unit". Die Autoren sehen das als Teil des _structural encodings_.
* Das Modell liefert als Erklärung für kontextuelle Effekte das Priming von PINs. Die beobachtbaren Kontexteffekte sind jedoch sehr vielseitig und könnten spezifischer ins Modell integriert werden.

# Gesichtswiedererkennung

Der [**Inversionseffekt (Face Inversion Effect)**](https://en.wikipedia.org/wiki/Face_inversion_effect) ist ein häufig verwendetes Instrument bei der Erforschung der Gesichtserkennung. Er kennzeichnet das Phänomen, dass das Erkennen eines Gesichts länger dauert, wenn es auf den Kopf gedreht ist (Yin, 1969). Bei anderen Objekten (z.B. Alltagsgegenständen) ist das nämlich nicht der Fall.

Biederman hat untersucht, ob die **Geons** der Objekterkennung auch für die Gesichtserkennung ausschlaggebend sind, kam aber eher zum gegenteiligen Schluss, nämlich dass sie keine wichtige Rolle dabei spielen.

In einem Experiment von Leder (1996) wurden Gesichtsfotos zu **Linienzeichnungen** transformiert. Das _structural encoding_ schien dafür nicht optimal ausgelegt zu sein, woraus man schlussfolgert, dass Gesichtserkennung nicht auf Linien als Informationsquelle basiert.

<a id="Gesichtskonfiguration">
Bei den Theorien zur Gesichtsverarbeitung gibt es Ansätze, die eine [_holistische Verarbeitung_](https://dorsch.hogrefe.com/stichwort/verarbeitung-holistische) annehmen und solche, die davon ausgehen, dass das Gesicht in seine Komponenten getrennt analysiert wird. Als konstituierende Elemente für Gesichter werden _Augen_, _Nase_ und _Mund_ angenommen. Ihre Abstände und Anordnung in einem Gesicht nennt man **Gesichtskonfiguration**. Eines dieser Elemente alleine (z.B. nur Augen) ist nicht hinreichend für die Wiedererkennung. Außerdem erklären sie den Inversionseffekt nicht, weil Augen, Nase und Mund auf den Kopf gedreht trotzdem sehr gut erkannt werden.<br>
Für eine holistische Verarbeitung spricht auch eine Studie von Tanakah und Farah (1993), in der zwei Gesichter bis auf die Nase identisch waren. Danach wurde untersucht, was besser unterschieden werden konnte: wenn man beide Gesichter sieht oder nur die Nase. Den Vpn fiel die Unterscheidung ganzer Gesichter leichter.

Manche Forschungen machen sich die Einprägsamkeit und Auffälligkeit (**Distinctiveness**) eines Gesichts zu Nutze. Leder und Bruce haben die Frage untersucht, was mehr Einfluss auf Distinctiveness und den Inversionseffekt hat: (1) die Gesichtskonfiguration oder (2) die Form und das Aussehen der lokalen Komponenten (Nase, Mund...). Sie haben Gesichter genommen, die in vorhergehenden Studien als durchschnittlich "distinctive" bewertet wurden und diese dann am Computer auf zwei Arten verändert, mit dem Ziel, dass sie mehr "distinctive" werden: Einerseits nur die Konfiguration, aber nicht die Komponenten selbst, und andererseits nur die Komponenten bei gleichbleibender Konfiguration. Danach haben die Vpn die Distinctiveness der Gesichter bewertet. Beide Arten der künstlich veränderten Gesichter waren in aufrechter Position mehr distinctive. Auf den Kopf gestellt verlor das konfigural veränderte Gesicht jedoch viel mehr an Distinctiveness als jenes mit veränderten Komponenten. In nachfolgenden Studien konnte man den Effekt reproduzieren.

TODO: Principal components analysis (PCA) von Hancock, Bruce & Burton

Konfigurale Merkmale erklären nur einige Aspekte der Gesichtserkennung. Bereits bekannte Gesichter können auch dann wiedererkannt werden, wenn sie stark verzerrt sind. Hinzu kommt, dass verzerrte Versionen dieser Gesichter Adaptionseffekte haben, sodass man danach leichte Verzerrungen sogar gegenüber dem Original bevorzugt. Eine Erklärung dafür ist, dass wir **Prototypen** als kognitive Repräsentation bilden, also so etwas wie einen Durchschnitt über alle Wahrnehmungen, die wir von einem Gesicht machen. Zum Beispiel hat ein Algorithmus aus mehreren Fotos von Bill Clinton ein Average gebildet und dieses wurde dann von den Vpn als am meisten mit Bill Clinton assoziiert (im Vergleich zu den Fotos, die als Input für den Algorithmus dienten).

# Relevanz und Anwendungsbereiche

Ein Anwendungsgebiet für Gesichtserkennung und die Forschung dazu sind **Identifikationsparaden und Fahndungssysteme**. Trotz stetiger Verbesserungen haben Bilder von Überwachungskameras oft schlechte Auflösungen, sind farblos und zeigen die verdächtige Person in schlechtem Winkel. Hinzu kommt, dass Menschen zwar gut darin sind bekannte Gesichter, aber erstaunlich schlecht darin unbekannte Gesichter wiederzuerkennen. Für akademische Anwendungen wurde dafür der [Glasgow Face Matching Test (GFMT)](https://en.wikipedia.org/wiki/Glasgow_Face_Matching_Test) entwickelt. Selbst dann, wenn man das Zielgesicht neben anderen Gesichtern sieht und direkt vergleichen kann, erkennen nur 70% das richtige Gesicht. Diese schlechte Quote stellt Augenzeugenberichte und Gegenüberstellung in kriminalpolizeilichen Bereichen in Frage. Auch Kontrolleure bei der Einreise am Flughafen zeigten bei einer Studie eine 10% Fehlerquote (6% false negative / 14% false positive). Zusätzlich stellte sich heraus, dass Erfahrung und Anzahl der Dienstjahre keinen Einflusss auf die Fähigkeit zur Gesichtserkennung hatten und somit nicht erlern- oder trainierbar ist. \
Schönheitsoperationen können (klarerweise) die Wiedererkennung erschweren, bzw. die Gesichtsidentität verändern.

Ähnlich dazu haben Leder, Gerger und Forster (2011) erforscht, welche Effekte **Brillen** auf die Gesichtswahrnehmung haben. Effekte waren, dass Brillen...:
* die Vertrauenswürdigkeit heben
* weniger attraktiv machen
* die Wiedererkennung behindern
* Brillenträger als intelligenter eingeschätzt werden

Ein weiteres Anwendungsgebiet sind **Face-Head-Modelle** (nach Vetter und Banz). Dabei handelt es sich um ein vom Computer anhand zahlreicher Fotos berechnetes Durchschnittsgesicht (**Morphing**). Der Computer hat auch diverse Dimensionen für Mimik (Lachen, Stirn runzeln), Hautfarbe, Nase, Mund, Wangenknochen, Geschlecht, Alter... herausgerechnet. Somit kann der Computer anhand von zweidimensionalen Fotos ein dreidimensionales Modell eines Gesichts konstruieren, das dann auf den oben genannten Dimensionen beliebig variiert und animiert werden kann. Haupteinsatzzwecke dafür ist die Entertainment/Film Industrie.

Gesichter sollen zunehmend auch durch **automatisierte Gesichtserkennung** von Computern identifiziert werden können (derzeit sind Pupillen und Fingerabdrücke jedoch noch besser für die Identifikation geeignet). Ein Teilgebiet davon ist das Erkennen des Gesichtsausdrucks. Das [Facial Action Coding System (FACS)](https://en.wikipedia.org/wiki/Facial_Action_Coding_System) hilft beim Erforschen der Gesichtsmuskulatur und ihrer Rolle für Gesichtsausdrücke.

**Prosopagnosie** wurde erstmals 1947 von Bodamer beschrieben. Menschen mit Prosopagnosie können keine Gesichter identifizieren. Sie können aber erkennen, dass es sich prinzipiell um ein Gesicht handelt und dieses auch strukturell beschreiben. Prosopagnosie kann erworben oder angeboren sein. Sie scheint im Zusammenhang mit Läsionen im Temporallappen (z.B. Schlaganfall) zu stehen. Da diese Störung isoliert auftreten kann, nimmt man an, dass bestimmte Gehirnregionen dediziert der Gesichtserkennung dienen:
* Fusiformes face area (FFA)
* Superior temporal sulcus (STS)
* Occipital face area (OFA)
